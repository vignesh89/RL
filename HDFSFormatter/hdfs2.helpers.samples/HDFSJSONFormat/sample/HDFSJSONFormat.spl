/* Copyright (C) 2013-2014, International Business Machines Corporation  */
/* All Rights Reserved                                	                 */

namespace sample;
use com.ibm.streamsx.json::JSONToTuple;
use com.ibm.streamsx.json::TupleToJSON;
use com.ibm.streamsx.hdfs2.helpers::HDFSReadFormat;
use com.ibm.streamsx.hdfs2.helpers::HDFSFormatWrite;

/** Wrapper around JSONToTuple so that it accepts a type argument.
 * The JSONToTuple operator does not need a outType argument,
 * as it inspects it output stream type.  However, the decoder
 * passed into the composite needs to accept an output type,
 * so we wrap the operator.
 * @input inStream Stream of strings in JSON format.
 * @output outStream Stream of tuples corresponding to JSON input stream.
 * @param outType type of the output.
 */
composite JSONToTupleWrapper (input inStream;
	output outStream) {
	param 
	type $outType;
	graph
	stream<$outType> outStream = JSONToTuple(inStream) {
	}
	}
	
/** Composite demonstrating the use of the JSON with the formatters.
 * To setup for running this composite
 * - Get the streamsx.json toolkit from IBMStreams
 * - Copy data/inputForJSONRead.txt to hadoopHelperSamples in HDFS.  
 * 
 * This example shows how to use your own formatters to convert tuples
 * to and from rstrings.  The format we use in for this example is the 
 * JSON format. 
 * 
 * After running, in the hadoopBaseDir, you'll see the file jsonWriteTest.txt
 * which contains a json format of the tuples from the beacon operator.
 * <code>
 * \\\{"aString":"Number is 0","n":0\\\}
 * \\\{"aString":"Number is 1","n":1\\\}
 * \\\{"aString":"Number is 2","n":2\\\}
 * ...
 * </code>
 * 
 * In the local data directory, you'll see jsonReadTest.csv, which contains csv
 * versions of the tuples read from JSON format.
 * 
 * <code>
 * "Number is 0",1
 * "Number is 2",3
 * "Number is 4",5
 * </code>
 */
composite JSONMain {
param

// Modify this parameter if you wish to use a different
// HDFS for the reading and writings.
expression<rstring> $hadoopBaseDir: "hdfsHelperSamples_JSON/";
graph

// Generate some tuples.
stream<rstring aString, int32 n> testStream = Beacon() {
	param
	iterations: 100;
	output testStream:
		aString = "Number is "+(rstring)IterationCount(),
		n = (int32)IterationCount();
}

// Write those tuples to HDFS. After the application has run,
// verify the tuples are here.
() as sink = HDFSFormatWrite(testStream) {
	param
	file: $hadoopBaseDir+"jsonWriteTest.txt";
	formatter: TupleToJSON;
}

// The tuples are stored on HDFS in JSON format.  This composite
// expands to two operators--one that reads lines, and a second
// that converts the lines to tuples. 
stream<rstring aString, int64 n> readTest = HDFSReadFormat() {
	param 
	file: $hadoopBaseDir+"inputForJSONRead.txt";
	decoder: JSONToTupleWrapper;
	streamType: tuple<rstring aString,int64 n>;
	}

// Write the read tuples locally, so you can verify them.
() as JsonReadTest = FileSink(readTest) {
	param
	file: "jsonReadTest.cvs";
	format: csv;
}

}